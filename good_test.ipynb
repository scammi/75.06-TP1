{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "9582"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "import pandas \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pandas.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "bb = pandas.read_csv('/home/skamy/Proyectos/7506/nlp-getting-started/test.csv')\n",
    "\n",
    "# bb.tail(10)\n",
    "# bb.index\n",
    "# bb.columns \n",
    "# bb.describe()\n",
    "\n",
    "#Limpio los datos de NaN\n",
    "fl = bb.dropna(how='any')\n",
    "\n",
    "#Add new col with the word count of twits\n",
    "count = fl['text'].str.split().str.len()\n",
    "fl['word_count'] = count\n",
    "\n",
    "#gets all emails containing twitter handles\n",
    "fl = fl[fl['text'].str.contains(\"@\")]\n",
    "#create col with all the twitter handle\n",
    "fl['twitter_handle'] = fl['text'].str.extract('(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9-_]+)')\n",
    "\n",
    "##count total number of words\n",
    "text_counts = fl['text'].str.cat().split(' ')\n",
    "len(text_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "-            299\nlike         141\n&amp;        134\nget          101\nvia           99\nfire          86\ni'm           72\nit's          72\nwould         72\nnew           69\none           68\npeople        66\n2             62\n|             57\nemergency     54\nattack        53\n...           53\nfirst         51\nsuicide       49\nfull          49\ndon't         48\nstill         48\ndisaster      45\nvideo         44\npolice        44\ngoing         44\nmake          42\ngot           42\nfires         41\nlove          40\nbody          40\nlast          40\nburning       39\nus            39\nknow          39\nsee           38\ntwo           38\nnews          38\nstorm         37\nthink         37\nneed          36\neven          36\ngo            35\n3             35\ntime          35\nback          35\nnuclear       34\ncar           33\narmy          33\nright         33\ndtype: int64"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "\n",
    "#get the 100 top words most used\n",
    "words = ''.join(bb['text']).lower().split()\n",
    "words_serie = pandas.Series(words)\n",
    "\n",
    "# .value_counts()[:100]\n",
    "# test = dict(test)\n",
    "#remove stop words\n",
    "stop_words ={'their', 'then', 'not', 'ma', 'here', 'other', 'won', 'up', 'weren', 'being', 'we', 'those', 'an', 'them', 'which', 'him', 'so', 'yourselves', 'what', 'own', 'has', 'should', 'above', 'in', 'myself', 'against', 'that', 'before', 't', 'just', 'into', 'about', 'most', 'd', 'where', 'our', 'or', 'such', 'ours', 'of', 'doesn', 'further', 'needn', 'now', 'some', 'too', 'hasn', 'more', 'the', 'yours', 'her', 'below', 'same', 'how', 'very', 'is', 'did', 'you', 'his', 'when', 'few', 'does', 'down', 'yourself', 'i', 'do', 'both', 'shan', 'have', 'itself', 'shouldn', 'through', 'themselves', 'o', 'didn', 've', 'm', 'off', 'out', 'but', 'and', 'doing', 'any', 'nor', 'over', 'had', 'because', 'himself', 'theirs', 'me', 'by', 'she', 'whom', 'hers', 're', 'hadn', 'who', 'he', 'my', 'if', 'will', 'are', 'why', 'from', 'am', 'with', 'been', 'its', 'ourselves', 'ain', 'couldn', 'a', 'aren', 'under', 'll', 'on', 'y', 'can', 'they', 'than', 'after', 'wouldn', 'each', 'once', 'mightn', 'for', 'this', 'these', 's', 'only', 'haven', 'having', 'all', 'don', 'it', 'there', 'until', 'again', 'to', 'while', 'be', 'no', 'during', 'herself', 'as', 'mustn', 'between', 'was', 'at', 'your', 'were', 'isn', 'wasn'}\n",
    "# rpt[rpt['STK_ID'].isin(stk_list)]\n",
    "# test = pandas.DataFrame(test)\n",
    "# test\n",
    "\n",
    "to_be_removed = []\n",
    "key = 0\n",
    "\n",
    "for word in words_serie:\n",
    "    for st_word in stop_words:\n",
    "        if word == st_word:\n",
    "            to_be_removed.append(key)    \n",
    "    key = key + 1\n",
    "\n",
    "words_serie = words_serie.drop(to_be_removed)\n",
    "\n",
    "words_serie = words_serie.value_counts()[:100]\n",
    "\n",
    "words_serie.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "New York                          38\nUSA                               37\nWorldwide                         16\nUnited States                     15\nCanada                            13\n                                  ..\nCentral New Jersey                 1\n36702 State Road 52, Dade City     1\nLansing, MI USA                    1\nToronto, Ontario, Canada           1\nIn A Multi-Fandom                  1\nName: location, Length: 1602, dtype: int64"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "location = pandas.Series(bb[\"location\"])\n",
    "location_count = location.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "deluged                 23\ndemolished              22\nrubble                  22\nsirens                  21\nfirst%20responders      21\nsnowstorm               21\nseismic                 21\nobliteration            21\nannihilation            21\nbioterrorism            20\nsurvivors               20\ncatastrophic            20\nmayhem                  20\navalanche               20\ndead                    20\ntrauma                  19\ncrushed                 19\nwild%20fires            19\ndevastated              19\nhostage                 19\nsuicide%20bomber        19\nobliterate              19\nobliterated             19\nterrorist               19\nelectrocute             18\ncyclone                 18\nforest%20fires          18\nhijacking               18\neyewitness              18\narson                   18\ndrown                   18\nblight                  18\ndestroyed               18\ntrapped                 18\napocalypse              18\ndetonation              18\nhailstorm               18\nsurvive                 18\ncrash                   17\nemergency%20services    17\nwounds                  17\ntrouble                 17\nbody%20bag              17\nthunderstorm            17\nsurvived                17\nmeltdown                17\ninjuries                17\nsuicide%20bombing       17\nlandslide               17\nwildfire                17\nName: keyword, dtype: int64"
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "key_words = pandas.Series(bb[\"keyword\"])\n",
    "key_words_count = key_words.value_counts()[:100]\n",
    "key_words_count.head(50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bit543c135b759447babc52ca75696ca469",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}